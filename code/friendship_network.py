# -*- coding: utf-8 -*-
"""Friendship Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yBXAfdpB6ob-nc1Rq7ozsE28WXvOIwnk
"""

pip install gdown

import gdown

# Download files from Google Drive
gdown.download("https://drive.google.com/uc?id=1ZZ_1ZYOtAvXXwAZc1k_T-as-KIthKF9S", "PP_recipes.csv", quiet=False)
gdown.download("https://drive.google.com/uc?id=1LcoQ9VliSLFZ9JxafuPzAG4a8pF9imCA", "PP_users.csv", quiet=False)
gdown.download("https://drive.google.com/uc?id=1tVqhPiTc73AAZlZASs4Pal3qly8aAxCr", "RAW_interactions.csv", quiet=False)

import pandas as pd
import numpy as np
import networkx as nx
import matplotlib
import matplotlib.pyplot as plt
import os
import pickle
from collections import defaultdict
import community as community_louvain
import matplotlib.cm as cm
import seaborn as sns
from tqdm import tqdm
import time

# Loading CSV files

def load_user_data(filepath):
    """Load user data from CSV file"""
    return pd.read_csv(filepath)


def load_recipe_data(filepath):
    """Load recipe data from CSV file"""
    return pd.read_csv(filepath)


def load_interaction_data(filepath):
    """Load interaction data from CSV file"""
    return pd.read_csv(filepath)

def create_recipe_to_users_mapping(users_df):
    """Create a mapping from recipes to users who interacted with them"""
    recipe_to_users = defaultdict(set)

    for idx, user_row in users_df.iterrows():
        user_id = user_row['u']
        items = eval(user_row['items'])  # Convert string representation of list to actual list

        # Add this user to the set of users for each recipe they interacted with
        for recipe_id in items:
            recipe_to_users[recipe_id].add(user_id)

    return recipe_to_users


def create_user_connections(recipe_to_users):
    """Create connections between users who share recipes"""
    user_connections = defaultdict(set)

    # For each recipe, create connections between all users who interacted with it
    for recipe_id, users in recipe_to_users.items():
        # If only one user interacted with this recipe, no connections to make
        if len(users) <= 1:
            continue

        # For each pair of users who interacted with this recipe, create a connection
        for user1 in users:
            for user2 in users:
                if user1 != user2:  # Avoid self-connections
                    user_connections[user1].add(user2)

    return user_connections

def build_friendship_graph(users_df, user_connections):
    """Build a NetworkX graph representing the friendship network"""
    G = nx.Graph()

    # Add all users as nodes
    all_users = set(users_df['u'])
    G.add_nodes_from(all_users)

    # Add edges for user connections
    for user1, connected_users in user_connections.items():
        for user2 in connected_users:
            G.add_edge(user1, user2)

    return G

def create_force_directed_layout(G):
    """Create and assign spring layout coordinates directly to graph"""
    print("Creating force-directed layout...")
    pos = nx.spring_layout(G, k=0.15, iterations=50)
     # Save the positions
    with open('force_directed_positions.pkl', 'wb') as f:
        pickle.dump(pos, f)

    return pos

def detect_communities(G, output_dir="communities/", rng_seed=55):
    """
    Detect communities using Louvain method for an unweighted graph.
    """
    import os
    os.makedirs(output_dir, exist_ok=True)

    print("Detecting communities with Louvain (unweighted)...")
    communities = nx.algorithms.community.louvain_communities(G, seed=rng_seed, weight=None)

    # Filter out singleton communities
    filtered_communities = [comm for comm in communities if len(comm) > 1]

    # Sort by size descending (largest first)
    filtered_communities.sort(key=len, reverse=True)

    partition = {}
    colors = plt.cm.get_cmap("tab20", len(filtered_communities))

    for i, comm in enumerate(filtered_communities):
        community_id = i + 1  # Start from 1
        rgba = tuple(float(c) for c in colors(i))
        for node in comm:
            partition[node] = community_id
            G.nodes[node]["rgba"] = rgba

        # Save this community subgraph
        comm_graph = G.subgraph(comm).copy()
        with open(os.path.join(output_dir, f"community_{community_id}.pkl"), "wb") as f:
            pickle.dump(comm_graph, f)

    # Save the partition dictionary
    with open(os.path.join(output_dir, "community_assignments.pkl"), "wb") as f:
        pickle.dump(partition, f)

    return partition

def detect_refined_communities(G):
    """Detect communities using Louvain algorithm and further subdivide large communities"""
    print("Detecting communities...")
    # First level community detection using Louvain algorithm
    partition = community_louvain.best_partition(G)

    # Count members in each community
    community_sizes = defaultdict(int)
    for node, community_id in partition.items():
        community_sizes[community_id] += 1

    large_community_threshold = 500

    # Further split large communities
    next_community_id = max(partition.values()) + 1

    for node in G.nodes():
        community_id = partition[node]

        # If node belongs to a large community, try to split further
        if community_sizes[community_id] > large_community_threshold:
            # Extract the subgraph of this community
            community_nodes = [n for n, c in partition.items() if c == community_id]
            subgraph = G.subgraph(community_nodes)

            # Apply Louvain again to this subgraph
            if len(community_nodes) > 2:
                sub_partition = community_louvain.best_partition(subgraph)

                # Remap the sub-community IDs to avoid conflicts
                for sub_node, sub_community in sub_partition.items():
                    partition[sub_node] = next_community_id + sub_community

                next_community_id += max(sub_partition.values()) + 1

    # Save the community assignments
    with open('refined_community_assignments.pkl', 'wb') as f:
        pickle.dump(partition, f)

    return partition

def visualize_communities(G, pos, partition):
    """Visualize communities with different colors using the force-directed layout"""
    print("Visualizing communities...")
    plt.figure(figsize=(16, 16))
    num_communities = len(set(partition.values()))
    cmap = plt.cm.get_cmap('tab20', num_communities)

    for community_id in set(partition.values()):
        community_nodes = [node for node, com in partition.items() if com == community_id]
        nx.draw_networkx_nodes(
            G,
            pos,
            nodelist=community_nodes,
            node_size=50,
            node_color=[cmap(community_id / num_communities)] * len(community_nodes),
            label=f"Community {community_id}"
        )

    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)

    community_sizes = defaultdict(int)
    for node, community_id in partition.items():
        community_sizes[community_id] += 1

    # Sort communities by size
    sorted_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)
    legend_labels = [f"Community {com}: {size} users" for com, size in sorted_communities]

    proxy_artists = [plt.Line2D([0], [0], marker='o', color=cmap(com / num_communities),
                              markersize=10, linestyle='')
                   for com, _ in sorted_communities]

    # Add legend
    if num_communities > 10:
        plt.legend(proxy_artists, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)
    else:
        plt.legend(proxy_artists, legend_labels, loc='lower center', ncol=3)

    plt.title("Friendship Network Communities", fontsize=20)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig('community_visualization.png', dpi=300, bbox_inches='tight')
    plt.close()

def calculate_community_centralities(G, partition):
    """Calculate centrality measures and other statistics for each community"""
    print("Calculating centrality measures..")
    community_stats = {}

    # Group nodes by community
    communities = defaultdict(list)
    for node, community_id in partition.items():
        communities[community_id].append(node)

    # For each community
    for community_id, members in tqdm(communities.items(), desc="Processing communities"):
        subgraph = G.subgraph(members)
        # Skip if community is too small
        if len(members) < 3:
            continue

        # Initialize statistics dictionary for this community
        stats = {
            'members': len(members),
            'edges': subgraph.number_of_edges(),
            'density': nx.density(subgraph),
            'clustering_coefficient': nx.average_clustering(subgraph),
            'global_transitivity': nx.transitivity(subgraph),
        }

        # Calculate degree centrality for all nodes
        degree_centrality = nx.degree_centrality(subgraph)
        stats['avg_degree_centrality'] = sum(degree_centrality.values()) / len(degree_centrality)
        stats['max_degree_centrality'] = max(degree_centrality.values())
        stats['degree_centrality_distribution'] = list(degree_centrality.values())

        # Calculate closeness centrality if the community is connected
        try:
            if nx.is_connected(subgraph) and len(members) < 1000:  # Avoid for very large communities
                closeness_centrality = nx.closeness_centrality(subgraph)
                stats['avg_closeness_centrality'] = sum(closeness_centrality.values()) / len(closeness_centrality)
                stats['max_closeness_centrality'] = max(closeness_centrality.values())
                stats['closeness_centrality_distribution'] = list(closeness_centrality.values())
            else:
                stats['avg_closeness_centrality'] = None
                stats['max_closeness_centrality'] = None
                stats['closeness_centrality_distribution'] = None
        except:
            stats['avg_closeness_centrality'] = None
            stats['max_closeness_centrality'] = None
            stats['closeness_centrality_distribution'] = None

        # Calculate betweenness centrality for smaller communities
        try:
            if len(members) < 500:  # Adjust this threshold based on your computational capacity
                betweenness_centrality = nx.betweenness_centrality(subgraph)
                stats['avg_betweenness_centrality'] = sum(betweenness_centrality.values()) / len(betweenness_centrality)
                stats['max_betweenness_centrality'] = max(betweenness_centrality.values())
                stats['betweenness_centrality_distribution'] = list(betweenness_centrality.values())
            else:
                # For larger communities, sample a subset of nodes for approximation
                sampled_nodes = np.random.choice(members, min(100, len(members)), replace=False)
                betweenness_centrality = nx.betweenness_centrality(subgraph, k=sampled_nodes)
                stats['avg_betweenness_centrality'] = sum(betweenness_centrality.values()) / len(betweenness_centrality)
                stats['max_betweenness_centrality'] = max(betweenness_centrality.values())
                stats['betweenness_centrality_distribution'] = list(betweenness_centrality.values())
                stats['betweenness_note'] = 'Approximated using node sampling'
        except:
            stats['avg_betweenness_centrality'] = None
            stats['max_betweenness_centrality'] = None
            stats['betweenness_centrality_distribution'] = None

        # Try to calculate average path length if connected
        try:
            if nx.is_connected(subgraph) and len(members) < 1000:
                stats['avg_path_length'] = nx.average_shortest_path_length(subgraph)
            else:
                stats['avg_path_length'] = None
        except:
            stats['avg_path_length'] = None

        # Store statistics for this community
        community_stats[community_id] = stats

    # Save the community statistics
    with open('community_statistics.pkl', 'wb') as f:
        pickle.dump(community_stats, f)

    return community_stats

def calculate_global_graph_metrics(G):
    """Calculate global metrics for the entire graph if it's connected"""
    print("Calculating global graph metrics...")
    metrics = {}

    if not nx.is_connected(G):
        print("Graph is not connected. Using largest component.")
        G = G.subgraph(max(nx.connected_components(G), key=len)).copy()

    try:
        metrics['avg_shortest_path_length'] = nx.average_shortest_path_length(G)
        metrics['diameter'] = nx.diameter(G)
        closeness = nx.closeness_centrality(G)
        metrics['avg_closeness'] = sum(closeness.values()) / len(closeness)
        betweenness = nx.betweenness_centrality(G)
        metrics['avg_betweenness'] = sum(betweenness.values()) / len(betweenness)
    except Exception as e:
        print("Could not compute some metrics:", e)

    with open("global_metrics.pkl", "wb") as f:
        pickle.dump(metrics, f)

    return metrics

def create_community_statistics_table(community_stats):
    """Create a table showing key statistics for each community"""
    print("Creating community statistics table...")
    table_data = []
    for community_id, stats in community_stats.items():
        if any(stats.get(k) is None for k in ['members', 'edges', 'density', 'clustering_coefficient']):
            continue

        row = {
            'Community': community_id,
            'Vertices': stats['members'],
            'Edges': stats['edges'],
            'Density': stats['density'],
            'Clustering Coefficient': stats['clustering_coefficient'],
            'Global Transitivity': stats['global_transitivity'],
            'Avg Degree Centrality': stats['avg_degree_centrality'],
            'Avg Path Length': stats.get('avg_path_length', 'N/A')
        }
        table_data.append(row)

    stats_df = pd.DataFrame(table_data)
    stats_df.to_csv('community_statistics_table.csv', index=False)

    plt.figure(figsize=(12, len(table_data) * 0.5 + 2))

    # Display table
    cell_text = []
    for _, row in stats_df.iterrows():
        cell_text.append([
            int(row['Community']),
            int(row['Vertices']),
            int(row['Edges']),
            f"{row['Density']:.4f}",
            f"{row['Average clustering Coefficient']:.4f}",
            f"{row['Global Transitivity']:.4f}",
            f"{row['Avg Path Length']:.4f}" if row['Avg Path Length'] != 'N/A' else 'N/A'
        ])

    # Create the table
    the_table = plt.table(
        cellText=cell_text,
        colLabels=stats_df.columns,
        loc='center',
        cellLoc='center'
    )

    # Adjust table appearance
    the_table.auto_set_font_size(False)
    the_table.set_fontsize(10)
    the_table.scale(1, 1.5)

    plt.axis('off')
    plt.title('Community Statistics', fontsize=16)
    plt.tight_layout()
    plt.savefig('community_statistics_table.png', dpi=300, bbox_inches='tight')
    plt.close()

    return stats_df

def analyze_shared_recipes(users_df, partition, recipe_to_users):
    """Analyze recipes shared within communities"""
    print("Analyzing shared recipes by community...")

    users_by_community = defaultdict(list)
    for user, community_id in partition.items():
        users_by_community[community_id].append(user)

    # Initialize data structures
    community_recipe_counts = defaultdict(int)  # Total recipe count by community
    community_top_recipes = defaultdict(lambda: defaultdict(int))  # Recipe frequencies by community

    # For each community, find recipes shared by its members
    for community_id, community_users in tqdm(users_by_community.items(), desc="Processing communities"):
        # Skip very small communities
        if len(community_users) < 3:
            continue

        all_community_recipes = set()
        for user_id in community_users:
            # Find this user in the users_df
            user_row = users_df[users_df['u'] == user_id]
            if not user_row.empty:
                items = eval(user_row['items'].iloc[0])
                all_community_recipes.update(items)

        recipe_user_counts = defaultdict(int)
        for recipe_id in all_community_recipes:
            users_with_recipe = recipe_to_users.get(recipe_id, set())
            community_users_with_recipe = set(community_users) & users_with_recipe
            num_community_users_with_recipe = len(community_users_with_recipe)

            if num_community_users_with_recipe >= 2:
                recipe_user_counts[recipe_id] = num_community_users_with_recipe
                community_recipe_counts[community_id] += 1

        top_recipes = sorted(recipe_user_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        for recipe_id, count in top_recipes:
            community_top_recipes[community_id][recipe_id] = count

    # Save results
    with open('community_recipe_counts.pkl', 'wb') as f:
        pickle.dump(community_recipe_counts, f)

    with open('community_top_recipes.pkl', 'wb') as f:
        pickle.dump(dict(community_top_recipes), f)

    return community_recipe_counts, community_top_recipes

def create_shared_recipes_histogram(community_recipe_counts):
    """Create histogram of shared recipes per community"""
    print("Creating shared recipes histogram...")
    # Filter out communities with no shared recipes
    filtered_counts = {k: v for k, v in community_recipe_counts.items() if v > 0}

    # Sort communities by number of shared recipes
    sorted_communities = sorted(filtered_counts.items(), key=lambda x: x[1], reverse=True)

    communities = [f"Comm {comm}" for comm, _ in sorted_communities]
    recipe_counts = [count for _, count in sorted_communities]


    plt.figure(figsize=(14, 8))
    bars = plt.bar(communities, recipe_counts, color='skyblue')
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{int(height)}', ha='center', va='bottom', rotation=0)

    plt.title('Number of Shared Recipes by Community', fontsize=16)
    plt.xlabel('Community', fontsize=14)
    plt.ylabel('Number of Shared Recipes', fontsize=14)
    plt.xticks(rotation=90)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    plt.savefig('shared_recipes_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()

def analyze_top_recipes(recipes_df, community_top_recipes):
    """Analyze top shared recipes within communities"""
    print("Analyzing top shared recipes...")
    # Prepare data structure for the analysis
    recipe_analysis = defaultdict(dict)

    # For each community
    for community_id, top_recipes in community_top_recipes.items():
        # Skip if no recipes
        if not top_recipes:
            continue

        community_analysis = []

        # Analyze each top recipe
        for recipe_id, share_count in top_recipes.items():
            # Find this recipe in recipes_df
            recipe_row = recipes_df[recipes_df['id'] == recipe_id]

            if recipe_row.empty:
                continue

            recipe_info = {
                'recipe_id': recipe_id,
                'recipe_name': 'Recipe ' + str(recipe_id),  # Since we don't have the actual name
                'share_count': share_count,
                'calorie_level': recipe_row['calorie_level'].iloc[0] if 'calorie_level' in recipe_row.columns else 'Unknown',
            }

            if 'ingredient_ids' in recipe_row.columns:
                try:
                    ingredients = eval(recipe_row['ingredient_ids'].iloc[0])
                    recipe_info['ingredient_count'] = len(ingredients)
                except:
                    recipe_info['ingredient_count'] = 0
            else:
                recipe_info['ingredient_count'] = 0

            if 'steps_tokens' in recipe_row.columns:
                try:
                    steps = eval(recipe_row['steps_tokens'].iloc[0])
                    recipe_info['steps_count'] = len(steps)
                except:
                    recipe_info['steps_count'] = 0
            else:
                recipe_info['steps_count'] = 0

            community_analysis.append(recipe_info)

        recipe_analysis[community_id] = community_analysis

    with open('top_recipes_analysis.pkl', 'wb') as f:
        pickle.dump(dict(recipe_analysis), f)

    return recipe_analysis

def create_top_recipes_charts(recipe_analysis):
    """Create charts for top recipes in each community"""
    print("Creating top recipes charts...")

    for community_id, recipes in recipe_analysis.items():
        if not recipes:
            continue

        # Sort recipes by share count
        sorted_recipes = sorted(recipes, key=lambda x: x['share_count'], reverse=True)

        fig, axes = plt.subplots(3, 1, figsize=(12, 18))

        recipe_names = [f"Recipe {r['recipe_id']}" for r in sorted_recipes]
        share_counts = [r['share_count'] for r in sorted_recipes]
        calorie_levels = [r['calorie_level'] if isinstance(r['calorie_level'], (int, float)) else 0 for r in sorted_recipes]
        ingredient_counts = [r['ingredient_count'] for r in sorted_recipes]
        steps_counts = [r['steps_count'] for r in sorted_recipes]

        max_shares = max(share_counts)
        colors = [plt.cm.YlOrRd(count / max_shares) for count in share_counts]

        # Plot calorie levels
        axes[0].bar(recipe_names, calorie_levels, color=colors)
        axes[0].set_title(f'Calorie Levels of Top Recipes in Community {community_id}', fontsize=14)
        axes[0].set_ylabel('Calorie Level', fontsize=12)
        axes[0].set_xticklabels(recipe_names, rotation=45, ha='right')
        axes[0].grid(axis='y', linestyle='--', alpha=0.7)

        # Plot ingredient counts
        axes[1].bar(recipe_names, ingredient_counts, color=colors)
        axes[1].set_title(f'Number of Ingredients in Top Recipes of Community {community_id}', fontsize=14)
        axes[1].set_ylabel('Ingredient Count', fontsize=12)
        axes[1].set_xticklabels(recipe_names, rotation=45, ha='right')
        axes[1].grid(axis='y', linestyle='--', alpha=0.7)

        # Plot steps counts
        axes[2].bar(recipe_names, steps_counts, color=colors)
        axes[2].set_title(f'Number of Steps in Top Recipes of Community {community_id}', fontsize=14)
        axes[2].set_ylabel('Steps Count', fontsize=12)
        axes[2].set_xticklabels(recipe_names, rotation=45, ha='right')
        axes[2].grid(axis='y', linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.savefig(f'community_{community_id}_top_recipes.png', dpi=300, bbox_inches='tight')
        plt.close()

def main():
    """Main function to orchestrate the entire analysis pipeline"""
    print("Starting friendship network analysis...")

    # Step 1: Load data
    users_df = load_user_data('PP_users.csv')
    recipes_df = load_recipe_data('PP_recipes.csv')
    interactions_df = load_interaction_data('RAW_interactions.csv')

    # Step 2: Create mappings and connections
    recipe_to_users = create_recipe_to_users_mapping(users_df)
    user_connections = create_user_connections(recipe_to_users)

    # Step 3: Build the friendship network
    G = build_friendship_graph(users_df, user_connections)
    print(f"Friendship network created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")

    # Save the graph
    with open('friendship_network.pkl', 'wb') as f:
        pickle.dump(G, f)

    # Step 4: Create force-directed layout
    if os.path.exists('force_directed_positions.pkl'):
        print("Loading precomputed layout...")
        with open('force_directed_positions.pkl', 'rb') as f:
            pos = pickle.load(f)
    else:
        pos = create_force_directed_layout(G)


    # Step 5: Detect communities
    partition_path = os.path.join("communities", "community_assignments.pkl")
    if os.path.exists(partition_path):
        print("Loading precomputed community assignments...")
        with open(partition_path, 'rb') as f:
            partition = pickle.load(f)
    else:
        partition = detect_refined_communities(G)
        # partition = detect_communities(G, output_dir="communities/", rng_seed=42)

    print(f"Detected {len(set(partition.values()))} communities")

    # Step 6: Visualize communities
    visualize_communities(G, pos, partition)

    # Step 7: Calculate community & global statistics
    community_stats = calculate_community_centralities(G, partition)

    # Step 8: Create community statistics table
    stats_df = create_community_statistics_table(community_stats)

    # Step 9: Analyze shared recipes
    community_recipe_counts, community_top_recipes = analyze_shared_recipes(users_df, partition, recipe_to_users)

    # Step 10: Create shared recipes histogram
    create_shared_recipes_histogram(community_recipe_counts)

    # Step 11: Analyze top recipes
    recipe_analysis = analyze_top_recipes(recipes_df, community_top_recipes)

    # Step 12: Create top recipes charts
    create_top_recipes_charts(recipe_analysis)

    print("Analysis complete!")
    return G, partition, community_stats, recipe_analysis


if __name__ == "__main__":
    G, partition, community_stats, recipe_analysis = main()