# -*- coding: utf-8 -*-
"""Friendship Network.ipynb

Automatically generated by Colab.]
"""

pip install gdown

import gdown

# Download files from Google Drive
gdown.download("https://drive.google.com/uc?id=1ZZ_1ZYOtAvXXwAZc1k_T-as-KIthKF9S", "PP_recipes.csv", quiet=False)
gdown.download("https://drive.google.com/uc?id=1LcoQ9VliSLFZ9JxafuPzAG4a8pF9imCA", "PP_users.csv", quiet=False)
gdown.download("https://drive.google.com/uc?id=1tVqhPiTc73AAZlZASs4Pal3qly8aAxCr", "RAW_interactions.csv", quiet=False)

import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import pickle
from collections import defaultdict
import community as community_louvain
import matplotlib.cm as cm
import seaborn as sns
from tqdm import tqdm

# Loading CSV files

def load_user_data(filepath):
    """Load user data from CSV file"""
    return pd.read_csv(filepath)


def load_recipe_data(filepath):
    """Load recipe data from CSV file"""
    return pd.read_csv(filepath)


def load_interaction_data(filepath):
    """Load interaction data from CSV file"""
    return pd.read_csv(filepath)

def create_recipe_to_users_mapping(users_df):
    """Create a mapping from recipes to users who interacted with them"""
    recipe_to_users = defaultdict(set)

    for idx, user_row in users_df.iterrows():
        user_id = user_row['u']
        items = eval(user_row['items'])  # Convert string representation of list to actual list

        # Add this user to the set of users for each recipe they interacted with
        for recipe_id in items:
            recipe_to_users[recipe_id].add(user_id)

    return recipe_to_users


def create_user_connections(recipe_to_users):
    """Create connections between users who share recipes"""
    user_connections = defaultdict(set)

    # For each recipe, create connections between all users who interacted with it
    for recipe_id, users in recipe_to_users.items():
        # If only one user interacted with this recipe, no connections to make
        if len(users) <= 1:
            continue

        # For each pair of users who interacted with this recipe, create a connection
        for user1 in users:
            for user2 in users:
                if user1 != user2:  # Avoid self-connections
                    user_connections[user1].add(user2)

    return user_connections

def build_friendship_graph(users_df, user_connections):
    """Build a NetworkX graph representing the friendship network"""
    G = nx.Graph()

    # Add all users as nodes
    all_users = set(users_df['u'])
    G.add_nodes_from(all_users)

    # Add edges for user connections
    for user1, connected_users in user_connections.items():
        for user2 in connected_users:
            G.add_edge(user1, user2)

    return G

def create_force_directed_layout(G):
    """Create and save a force-directed layout using spring layout"""
    print("Creating force-directed layout...")
    # Create spring layout
    pos = nx.spring_layout(G, k=0.15, iterations=50)

    # Save the positions
    with open('force_directed_positions.pkl', 'wb') as f:
        pickle.dump(pos, f)

    return pos

def detect_communities(G):
    """Detect communities using Louvain algorithm and further subdivide large communities"""
    print("Detecting communities...")
    # First level community detection using Louvain algorithm
    partition = community_louvain.best_partition(G)

    # Count members in each community
    community_sizes = defaultdict(int)
    for node, community_id in partition.items():
        community_sizes[community_id] += 1

    # Define threshold for "large" communities that need further splitting
    large_community_threshold = 500  # Adjust this based on your computational capacity

    # Further split large communities
    next_community_id = max(partition.values()) + 1

    for node in G.nodes():
        community_id = partition[node]

        # If node belongs to a large community, try to split further
        if community_sizes[community_id] > large_community_threshold:
            # Extract the subgraph of this community
            community_nodes = [n for n, c in partition.items() if c == community_id]
            subgraph = G.subgraph(community_nodes)

            # Apply Louvain again to this subgraph
            if len(community_nodes) > 2:  # Need at least 3 nodes for meaningful community detection
                sub_partition = community_louvain.best_partition(subgraph)

                # Remap the sub-community IDs to avoid conflicts
                for sub_node, sub_community in sub_partition.items():
                    partition[sub_node] = next_community_id + sub_community

                next_community_id += max(sub_partition.values()) + 1

    # Save the community assignments
    with open('community_assignments.pkl', 'wb') as f:
        pickle.dump(partition, f)

    return partition

def visualize_communities(G, pos, partition):
    """Visualize communities with different colors using the force-directed layout"""
    print("Visualizing communities...")
    # Create a figure with a specific axis
    plt.figure(figsize=(16, 16))

    # Count number of communities
    num_communities = len(set(partition.values()))

    # Choose a colormap with enough distinct colors
    cmap = cm.get_cmap('tab20', num_communities)

    # Draw nodes
    for community_id in set(partition.values()):
        # Get list of nodes in this community
        community_nodes = [node for node, com in partition.items() if com == community_id]

        # Draw these nodes with a specific color
        nx.draw_networkx_nodes(
            G,
            pos,
            nodelist=community_nodes,
            node_size=50,
            node_color=[cmap(community_id / num_communities)] * len(community_nodes),
            label=f"Community {community_id}"
        )

    # Draw edges with transparency
    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)

    # Create custom legend with community sizes
    community_sizes = defaultdict(int)
    for node, community_id in partition.items():
        community_sizes[community_id] += 1

    # Sort communities by size for the legend
    sorted_communities = sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)
    legend_labels = [f"Community {com}: {size} users" for com, size in sorted_communities]

    # Use proxy artists for the legend
    proxy_artists = [plt.Line2D([0], [0], marker='o', color=cmap(com / num_communities),
                              markersize=10, linestyle='')
                   for com, _ in sorted_communities]

    # Add legend (place it outside the main plot area if too many communities)
    if num_communities > 10:
        plt.legend(proxy_artists, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)
    else:
        plt.legend(proxy_artists, legend_labels, loc='lower center', ncol=3)

    plt.title("Friendship Network Communities", fontsize=20)
    plt.axis('off')
    plt.tight_layout()
    plt.savefig('community_visualization.png', dpi=300, bbox_inches='tight')
    plt.close()

    # Also create a more simplified version with just colors if there are many communities
    if num_communities > 10:
        plt.figure(figsize=(16, 16))
        # Draw nodes colored by community
        node_colors = [cmap(partition[node] / num_communities) for node in G.nodes()]
        nx.draw_networkx_nodes(G, pos, node_size=50, node_color=node_colors)
        nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)
        plt.title("Friendship Network Communities (Simplified View)", fontsize=20)
        plt.axis('off')
        plt.tight_layout()
        plt.savefig('community_visualization_simple.png', dpi=300)
        plt.close()

def calculate_community_centralities(G, partition):
    """Calculate centrality measures and other statistics for each community"""
    print("Calculating centrality measures...")
    community_stats = {}

    # Group nodes by community
    communities = defaultdict(list)
    for node, community_id in partition.items():
        communities[community_id].append(node)

    # For each community
    for community_id, members in tqdm(communities.items(), desc="Processing communities"):
        # Extract the subgraph for this community
        subgraph = G.subgraph(members)

        # Skip if community is too small for meaningful analysis
        if len(members) < 3:
            continue

        # Initialize statistics dictionary for this community
        stats = {
            'members': len(members),
            'edges': subgraph.number_of_edges(),
            'density': nx.density(subgraph),
            'clustering_coefficient': nx.average_clustering(subgraph),
            'global_transitivity': nx.transitivity(subgraph),
        }

        # Calculate degree centrality for all nodes
        degree_centrality = nx.degree_centrality(subgraph)
        stats['avg_degree_centrality'] = sum(degree_centrality.values()) / len(degree_centrality)
        stats['max_degree_centrality'] = max(degree_centrality.values())
        stats['degree_centrality_distribution'] = list(degree_centrality.values())

        # Calculate closeness centrality if the community is connected
        try:
            if nx.is_connected(subgraph) and len(members) < 1000:  # Avoid for very large communities
                closeness_centrality = nx.closeness_centrality(subgraph)
                stats['avg_closeness_centrality'] = sum(closeness_centrality.values()) / len(closeness_centrality)
                stats['max_closeness_centrality'] = max(closeness_centrality.values())
                stats['closeness_centrality_distribution'] = list(closeness_centrality.values())
            else:
                stats['avg_closeness_centrality'] = None
                stats['max_closeness_centrality'] = None
                stats['closeness_centrality_distribution'] = None
        except:
            stats['avg_closeness_centrality'] = None
            stats['max_closeness_centrality'] = None
            stats['closeness_centrality_distribution'] = None

        # Calculate betweenness centrality for smaller communities (it's computationally expensive)
        try:
            if len(members) < 500:  # Adjust this threshold based on your computational capacity
                betweenness_centrality = nx.betweenness_centrality(subgraph)
                stats['avg_betweenness_centrality'] = sum(betweenness_centrality.values()) / len(betweenness_centrality)
                stats['max_betweenness_centrality'] = max(betweenness_centrality.values())
                stats['betweenness_centrality_distribution'] = list(betweenness_centrality.values())
            else:
                # For larger communities, sample a subset of nodes for approximation
                sampled_nodes = np.random.choice(members, min(100, len(members)), replace=False)
                betweenness_centrality = nx.betweenness_centrality(subgraph, k=sampled_nodes)
                stats['avg_betweenness_centrality'] = sum(betweenness_centrality.values()) / len(betweenness_centrality)
                stats['max_betweenness_centrality'] = max(betweenness_centrality.values())
                stats['betweenness_centrality_distribution'] = list(betweenness_centrality.values())
                stats['betweenness_note'] = 'Approximated using node sampling'
        except:
            stats['avg_betweenness_centrality'] = None
            stats['max_betweenness_centrality'] = None
            stats['betweenness_centrality_distribution'] = None

        # Try to calculate average path length if connected
        try:
            if nx.is_connected(subgraph) and len(members) < 1000:
                stats['avg_path_length'] = nx.average_shortest_path_length(subgraph)
            else:
                stats['avg_path_length'] = None
        except:
            stats['avg_path_length'] = None

        # Store statistics for this community
        community_stats[community_id] = stats

    # Save the community statistics
    with open('community_statistics.pkl', 'wb') as f:
        pickle.dump(community_stats, f)

    return community_stats

def create_community_statistics_table(community_stats):
    """Create a table showing key statistics for each community"""
    print("Creating community statistics table...")
    # Prepare data for the table
    table_data = []

    for community_id, stats in community_stats.items():
        # Skip communities with missing key data
        if any(stats.get(k) is None for k in ['members', 'edges', 'density', 'clustering_coefficient']):
            continue

        row = {
            'Community': community_id,
            'Vertices': stats['members'],
            'Edges': stats['edges'],
            'Density': stats['density'],
            'Clustering Coefficient': stats['clustering_coefficient'],
            'Global Transitivity': stats['global_transitivity'],
            'Avg Degree Centrality': stats['avg_degree_centrality'],
            'Avg Path Length': stats.get('avg_path_length', 'N/A')
        }
        table_data.append(row)

    # Create DataFrame and save as CSV
    stats_df = pd.DataFrame(table_data)
    stats_df.to_csv('community_statistics_table.csv', index=False)

    # Also create a visual table using matplotlib for the report
    plt.figure(figsize=(12, len(table_data) * 0.5 + 2))

    # Display table
    cell_text = []
    for _, row in stats_df.iterrows():
        cell_text.append([
            row['Community'],
            f"{row['Vertices']:,}",
            f"{row['Edges']:,}",
            f"{row['Density']:.4f}",
            f"{row['Clustering Coefficient']:.4f}",
            f"{row['Global Transitivity']:.4f}",
            f"{row['Avg Degree Centrality']:.4f}",
            str(row['Avg Path Length']) if row['Avg Path Length'] != 'N/A' else 'N/A'
        ])

    # Create the table
    the_table = plt.table(
        cellText=cell_text,
        colLabels=stats_df.columns,
        loc='center',
        cellLoc='center'
    )

    # Adjust table appearance
    the_table.auto_set_font_size(False)
    the_table.set_fontsize(10)
    the_table.scale(1, 1.5)

    # Hide axes
    plt.axis('off')
    plt.title('Community Statistics', fontsize=16)
    plt.tight_layout()
    plt.savefig('community_statistics_table.png', dpi=300, bbox_inches='tight')
    plt.close()

    return stats_df

def analyze_shared_recipes(users_df, partition, recipe_to_users):
    """Analyze recipes shared within communities"""
    print("Analyzing shared recipes by community...")
    # Group users by community
    users_by_community = defaultdict(list)
    for user, community_id in partition.items():
        users_by_community[community_id].append(user)

    # Initialize data structures
    community_recipe_counts = defaultdict(int)  # Total recipe count by community
    community_top_recipes = defaultdict(lambda: defaultdict(int))  # Recipe frequencies by community

    # For each community, find recipes shared by its members
    for community_id, community_users in tqdm(users_by_community.items(), desc="Processing communities"):
        # Skip very small communities
        if len(community_users) < 3:
            continue

        # Find all recipes interacted with by any user in this community
        all_community_recipes = set()
        for user_id in community_users:
            # Find this user in the users_df
            user_row = users_df[users_df['u'] == user_id]
            if not user_row.empty:
                items = eval(user_row['items'].iloc[0])
                all_community_recipes.update(items)

        # Count how many users in this community interacted with each recipe
        recipe_user_counts = defaultdict(int)
        for recipe_id in all_community_recipes:
            users_with_recipe = recipe_to_users.get(recipe_id, set())
            community_users_with_recipe = set(community_users) & users_with_recipe
            num_community_users_with_recipe = len(community_users_with_recipe)

            # Only count recipes shared by at least 2 users in the community
            if num_community_users_with_recipe >= 2:
                recipe_user_counts[recipe_id] = num_community_users_with_recipe
                community_recipe_counts[community_id] += 1

        # Get top 10 most shared recipes in this community
        top_recipes = sorted(recipe_user_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        for recipe_id, count in top_recipes:
            community_top_recipes[community_id][recipe_id] = count

    # Save results
    with open('community_recipe_counts.pkl', 'wb') as f:
        pickle.dump(community_recipe_counts, f)

    with open('community_top_recipes.pkl', 'wb') as f:
        pickle.dump(dict(community_top_recipes), f)

    return community_recipe_counts, community_top_recipes

def create_shared_recipes_histogram(community_recipe_counts):
    """Create histogram of shared recipes per community"""
    print("Creating shared recipes histogram...")
    # Filter out communities with no shared recipes
    filtered_counts = {k: v for k, v in community_recipe_counts.items() if v > 0}

    # Sort communities by number of shared recipes
    sorted_communities = sorted(filtered_counts.items(), key=lambda x: x[1], reverse=True)

    # Prepare data for plotting
    communities = [f"Comm {comm}" for comm, _ in sorted_communities]
    recipe_counts = [count for _, count in sorted_communities]

    # Create the plot
    plt.figure(figsize=(14, 8))
    bars = plt.bar(communities, recipe_counts, color='skyblue')

    # Add data labels on top of each bar
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{int(height)}', ha='center', va='bottom', rotation=0)

    plt.title('Number of Shared Recipes by Community', fontsize=16)
    plt.xlabel('Community', fontsize=14)
    plt.ylabel('Number of Shared Recipes', fontsize=14)
    plt.xticks(rotation=90)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    # Save the figure
    plt.savefig('shared_recipes_histogram.png', dpi=300, bbox_inches='tight')
    plt.close()

def analyze_top_recipes(recipes_df, community_top_recipes):
    """Analyze top shared recipes within communities"""
    print("Analyzing top shared recipes...")
    # Prepare data structure for the analysis
    recipe_analysis = defaultdict(dict)

    # For each community
    for community_id, top_recipes in community_top_recipes.items():
        # Skip if no recipes
        if not top_recipes:
            continue

        community_analysis = []

        # Analyze each top recipe
        for recipe_id, share_count in top_recipes.items():
            # Find this recipe in recipes_df
            recipe_row = recipes_df[recipes_df['id'] == recipe_id]

            if recipe_row.empty:
                continue

            # Extract recipe information
            recipe_info = {
                'recipe_id': recipe_id,
                'recipe_name': 'Recipe ' + str(recipe_id),  # Since we don't have the actual name
                'share_count': share_count,
                'calorie_level': recipe_row['calorie_level'].iloc[0] if 'calorie_level' in recipe_row.columns else 'Unknown',
            }

            # Count ingredients if available
            if 'ingredient_ids' in recipe_row.columns:
                try:
                    ingredients = eval(recipe_row['ingredient_ids'].iloc[0])
                    recipe_info['ingredient_count'] = len(ingredients)
                except:
                    recipe_info['ingredient_count'] = 0
            else:
                recipe_info['ingredient_count'] = 0

            # Count steps if available
            if 'steps_tokens' in recipe_row.columns:
                try:
                    steps = eval(recipe_row['steps_tokens'].iloc[0])
                    recipe_info['steps_count'] = len(steps)
                except:
                    recipe_info['steps_count'] = 0
            else:
                recipe_info['steps_count'] = 0

            community_analysis.append(recipe_info)

        recipe_analysis[community_id] = community_analysis

    # Save the analysis results
    with open('top_recipes_analysis.pkl', 'wb') as f:
        pickle.dump(dict(recipe_analysis), f)

    return recipe_analysis

def create_top_recipes_charts(recipe_analysis):
    """Create charts for top recipes in each community"""
    print("Creating top recipes charts...")
    # For each community with analyzed recipes
    for community_id, recipes in recipe_analysis.items():
        if not recipes:
            continue

        # Sort recipes by share count
        sorted_recipes = sorted(recipes, key=lambda x: x['share_count'], reverse=True)

        # Create figure with 3 subplots (kcal, ingredients, steps)
        fig, axes = plt.subplots(3, 1, figsize=(12, 18))

        # Prepare data
        recipe_names = [f"Recipe {r['recipe_id']}" for r in sorted_recipes]
        share_counts = [r['share_count'] for r in sorted_recipes]
        calorie_levels = [r['calorie_level'] if isinstance(r['calorie_level'], (int, float)) else 0 for r in sorted_recipes]
        ingredient_counts = [r['ingredient_count'] for r in sorted_recipes]
        steps_counts = [r['steps_count'] for r in sorted_recipes]

        # Create color mapping based on share count
        max_shares = max(share_counts)
        colors = [plt.cm.YlOrRd(count / max_shares) for count in share_counts]

        # Plot calorie levels
        axes[0].bar(recipe_names, calorie_levels, color=colors)
        axes[0].set_title(f'Calorie Levels of Top Recipes in Community {community_id}', fontsize=14)
        axes[0].set_ylabel('Calorie Level', fontsize=12)
        axes[0].set_xticklabels(recipe_names, rotation=45, ha='right')
        axes[0].grid(axis='y', linestyle='--', alpha=0.7)

        # Plot ingredient counts
        axes[1].bar(recipe_names, ingredient_counts, color=colors)
        axes[1].set_title(f'Number of Ingredients in Top Recipes of Community {community_id}', fontsize=14)
        axes[1].set_ylabel('Ingredient Count', fontsize=12)
        axes[1].set_xticklabels(recipe_names, rotation=45, ha='right')
        axes[1].grid(axis='y', linestyle='--', alpha=0.7)

        # Plot steps counts
        axes[2].bar(recipe_names, steps_counts, color=colors)
        axes[2].set_title(f'Number of Steps in Top Recipes of Community {community_id}', fontsize=14)
        axes[2].set_ylabel('Steps Count', fontsize=12)
        axes[2].set_xticklabels(recipe_names, rotation=45, ha='right')
        axes[2].grid(axis='y', linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.savefig(f'community_{community_id}_top_recipes.png', dpi=300, bbox_inches='tight')
        plt.close()

def calculate_network_statistics_bkp(G):
    """
    Calculate and display basic statistics about the network

    Parameters:
    -----------
    G : networkx.Graph
        Graph representation of the friendship network

    Returns:
    --------
    dict
        Dictionary containing network statistics
    """
    num_nodes = G.number_of_nodes()
    num_edges = G.number_of_edges()
    density = nx.density(G)

    try:
        avg_clustering = nx.average_clustering(G)
    except:
        avg_clustering = "Cannot calculate (possible division by zero)"

    connected_components = list(nx.connected_components(G))
    largest_component_size = max([len(component) for component in connected_components]) if connected_components else 0

    stats = {
        "num_nodes": num_nodes,
        "num_edges": num_edges,
        "density": density,
        "avg_clustering": avg_clustering,
        "num_connected_components": len(connected_components),
        "largest_component_size": largest_component_size
    }

    print(f"Network Statistics:")
    print(f"Number of nodes (users): {num_nodes}")
    print(f"Number of edges (friendships): {num_edges}")
    print(f"Network density: {density:.6f}")
    print(f"Average clustering coefficient: {avg_clustering}")
    print(f"Number of connected components: {len(connected_components)}")
    print(f"Size of largest connected component: {largest_component_size}")

    return stats

def visualize_network_bkp(G, output_file='friendship_network.png'):
    """
    Create and save a visualization of the friendship network

    Parameters:
    -----------
    G : networkx.Graph
        Graph representation of the friendship network
    output_file : str, optional
        Filename to save the visualization
    """
    # Create figure with a specific axis for the graph and space for the colorbar
    fig, ax = plt.subplots(figsize=(12, 12))

    # Use a layout algorithm suitable for larger graphs
    pos = nx.spring_layout(G, k=0.15, iterations=50)  # k controls the distance between nodes

    # Color nodes based on degree (number of connections)
    node_degrees = dict(G.degree())
    node_colors = [node_degrees[node] for node in G.nodes()]

    # Get the min and max for the colormap normalization
    vmin = min(node_colors) if node_colors else 0
    vmax = max(node_colors) if node_colors else 1

    # Draw the network on our specific axis
    nodes = nx.draw_networkx_nodes(
        G,
        pos=pos,
        ax=ax,
        node_color=node_colors,
        cmap=plt.cm.viridis,
        vmin=vmin,
        vmax=vmax,
        node_size=50
    )

    # Draw edges separately
    nx.draw_networkx_edges(
        G,
        pos=pos,
        ax=ax,
        edge_color='gray',
        alpha=0.7,
        width=0.5
    )

    # Now we can create a colorbar from the nodes mappable
    plt.colorbar(nodes, ax=ax, label="Number of Friends")

    ax.set_title("User Friendship Network (based on shared recipes)")
    ax.axis('off')  # Turn off axis

    plt.tight_layout()
    plt.savefig(output_file, dpi=300)
    plt.show()


def save_graph(G, output_file='friendship_network.gexf'):
    """
    Save the graph in GEXF format for further analysis

    Parameters:
    -----------
    G : networkx.Graph
        Graph representation of the friendship network
    output_file : str, optional
        Filename to save the graph
    """
    nx.write_gexf(G, output_file)

def main():
    """Main function to orchestrate the entire analysis pipeline"""
    print("Starting friendship network analysis...")

    # Step 1: Load data
    users_df = load_user_data('PP_users.csv')
    recipes_df = load_recipe_data('PP_recipes.csv')

    # Step 2: Create mappings and connections
    recipe_to_users = create_recipe_to_users_mapping(users_df)
    user_connections = create_user_connections(recipe_to_users)

    # Step 3: Build the friendship network
    G = build_friendship_graph(users_df, user_connections)
    print(f"Friendship network created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")

    # Save the graph
    with open('friendship_network.pkl', 'wb') as f:
        pickle.dump(G, f)

    # Step 4: Create force-directed layout
    pos = create_force_directed_layout(G)

    # Step 5: Detect communities
    partition = detect_communities(G)
    print(f"Detected {len(set(partition.values()))} communities")

    # Step 6: Visualize communities
    visualize_communities(G, pos, partition)

    # Step 7: Calculate community statistics
    community_stats = calculate_community_centralities(G, partition)

    # Step 8: Create community statistics table
    stats_df = create_community_statistics_table(community_stats)

    # Step 9: Analyze shared recipes
    community_recipe_counts, community_top_recipes = analyze_shared_recipes(users_df, partition, recipe_to_users)

    # Step 10: Create shared recipes histogram
    create_shared_recipes_histogram(community_recipe_counts)

    # Step 11: Analyze top recipes
    recipe_analysis = analyze_top_recipes(recipes_df, community_top_recipes)

    # Step 12: Create top recipes charts
    create_top_recipes_charts(recipe_analysis)

    print("Analysis complete! All results saved to files.")
    return G, partition, community_stats, recipe_analysis


if __name__ == "__main__":
    G, partition, community_stats, recipe_analysis = main()
